* Introduction and approach

** object detection for nuts
I use [[https://github.com/PramaLLC/BEN2][BEN2]] trained for [[https://paperswithcode.com/dataset/dis5k][Dichotomous Image Segmentation]] to segment the nuts from the clear background.
This works for segmenting in images and the video where the nut is in clean background
but does not work for other videos where there are significant parts around the nuts.

** Object tracking in videos
I was able to get moderate performance in tracking nuts in the clean video using the
[[https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2_t.pt][tiny model]] of [[https://docs.ultralytics.com/models/sam-2/][SAM-2.1 as implemented in YOLO library]].
Performance in other videos were not appropriate with tiny model and require larger models
which donot run fast enough.

* First, set up the venv
I am using [[https://github.com/astral-sh/uv][uv]] as it is very fast.

** Create venv
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  uv venv "${HOME}/BEN2"
#+end_src

** Activate venv
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  . "${HOME}/BEN2/bin/activate"
#+end_src

** Install other basic packages
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  uv pip install \
      "hydra-core" \
      "iopath" \
      "numpy" \
      "pillow" \
      "tqdm" \
      'ipython' \
      'jpeg4py' \
      'lmdb' \
      'loguru' \
      'matplotlib' \
      'opencv-python' \
      'pandas' \
      'scipy' \
      'tikzplotlib' \
      'ultralytics' \
  ;
#+end_src

** Install torch

*** For cpu
I used the CPU version of torch as I did not have access to GPU for this work,
but you can perhaps use GPU version for torch.
The CPU version is sufficient to run BEN2 and SAM-2.1 tiny models
at inference speeds of around 1 second per image.

#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  uv pip install \
      'torch' \
      'torchvision' \
      'torchaudio' \
      '--index-url' 'https://download.pytorch.org/whl/cpu' \
  ;
#+end_src

** Download and install BEN2

*** Function to download code from GITHUB
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  get_repo(){
      DIR_REPO="${HOME}/GITHUB/$('echo' "${1}" | 'sed' 's/^git@github.com://g ; s@^https://github.com/@@g ; s@.git$@@g' )"
      DIR_BASE="$('dirname' '--' "${DIR_REPO}")"

      mkdir -pv -- "${DIR_BASE}"
      cd "${DIR_BASE}"
      git clone "${1}"
      cd "${DIR_REPO}"

      if test "${#}" '-ge' '2'
      then
          git switch "${2}"
      else
          git switch main
      fi

      git pull
      git submodule update --recursive --init

      if test "${#}" '-ge' '3'
      then
          git checkout "${3}"
      fi
  }
#+end_src

*** Download and install BEN2
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  get_repo 'https://github.com/PramaLLC/BEN2.git'
  uv pip install .
#+end_src

** Download ultralytics sam checkpoints
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  mkdir -pv -- "${HOME}/MODEL_CHECKPOINTS/ULTRALYTICS/SAM2/"
  cd "${HOME}/MODEL_CHECKPOINTS/ULTRALYTICS/SAM2/"

  aria2c -c -x16 -j16 'https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_t.pt'
  aria2c -c -x16 -j16 'https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_s.pt'
  aria2c -c -x16 -j16 'https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_b.pt'
  aria2c -c -x16 -j16 'https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_l.pt'
#+end_src

** COMMENT Download SAM2
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  get_repo 'https://github.com/facebookresearch/sam2.git'

  cd "${HOME}/GITHUB/facebookresearch/sam2/checkpoints"
  (test -e sam2.1_hiera_base_plus.pt && test -e sam2.1_hiera_large.pt && test -e sam2.1_hiera_small.pt && test -e sam2.1_hiera_tiny.pt) || './download_ckpts.sh'
#+end_src

** COMMENT Download and install SAMURAI
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./venv_setup.sh
  get_repo 'https://github.com/yangchris11/samurai.git'
  cd sam2
  uv pip install .
  uv pip install ".[notebooks]"
  cd checkpoints
  ./download_ckpts.sh
#+end_src

* Extract and set up the images

** Extract the zip
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./extract.sh
  unzip 'Weldnut.zip'
#+end_src

** Some cleanups
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./extract.sh
  rm -vrf -- '__MACOSX'
#+end_src

** Organize images and videos
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./convert.sh
  mkdir -pv -- jpeg mov
  mv -vf -- 'Weldnut/IMG_0621 weldnut.jpeg' 'Weldnut/IMG_0628 weldnut.jpeg' 'Weldnut/IMG_0622 weldnut.jpeg' 'Weldnut/IMG_0623 weldnut.jpeg' 'Weldnut/IMG_0624 weldnut.jpeg' 'Weldnut/IMG_0625 weldnut.jpeg' 'Weldnut/IMG_0626 weldnut.jpeg' 'Weldnut/IMG_0627 weldnut.jpeg' 'jpeg'
  mv -vf -- 'Weldnut/IMG_0629 weldnut.MOV' 'Weldnut/IMG_0631-14 weldnut scan.MOV' 'Weldnut/IMG_0632-14 weldnut scan.MOV' 'Weldnut/IMG_0633-12 weldnut scan.MOV' 'Weldnut/IMG_0630-14 weldnut scan.MOV' 'mov'
#+end_src

** Convert jpeg to png to allow alpha info
This is done using [[http://www.graphicsmagick.org/][graphicsmagic]]
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./convert.sh
  mkdir -pv -- png
  convert './jpeg/IMG_0621 weldnut.jpeg' './png/IMG_0621 weldnut.png'
  convert './jpeg/IMG_0625 weldnut.jpeg' './png/IMG_0625 weldnut.png'
  convert './jpeg/IMG_0622 weldnut.jpeg' './png/IMG_0622 weldnut.png'
  convert './jpeg/IMG_0626 weldnut.jpeg' './png/IMG_0626 weldnut.png'
  convert './jpeg/IMG_0623 weldnut.jpeg' './png/IMG_0623 weldnut.png'
  convert './jpeg/IMG_0627 weldnut.jpeg' './png/IMG_0627 weldnut.png'
  convert './jpeg/IMG_0624 weldnut.jpeg' './png/IMG_0624 weldnut.png'
  convert './jpeg/IMG_0628 weldnut.jpeg' './png/IMG_0628 weldnut.png'
#+end_src

* Main code for segmenting the weldnut images and object detection

** COMMENT Shell script to 
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./segment.sh
  . "${HOME}/BEN2/bin/activate"
  python3 ./segment.py
#+end_src

** Main python code

*** Import the libraries
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  from PIL import Image
  from ben2 import BEN_Base
  import cv2
  import numpy as np
  import os
  import torch
#+end_src

** Create directory to store the output files
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  os.makedirs(name='./mask/',  exist_ok=True)
#+end_src

** Device to run the model on
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model = BEN_Base.from_pretrained("PramaLLC/BEN2")
  model.to(device).eval()
#+end_src

** Main function to do the segmentation
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  def slave(name):
      image = Image.open("./png/" + name)
      foreground = model.inference(
          image,
          refine_foreground=False,
      )  # Refine foreground is an extract postprocessing step that increases inference time but can improve matting edges. The default value is False.

      foreground.save("./mask/" + name)
#+end_src

** Segment all the input images
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  for i in os.listdir("./png/"):
      slave(name=i)
#+end_src

** Extract the mask from RGBA images
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  def process_segment(img):
      return (img[:, :, 3] > 127).astype(np.uint8)
#+end_src

** Simple check to see the masks are good
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  def visualize(name):
      orig = cv2.imread("./png/" + name, cv2.IMREAD_COLOR)
      segm = cv2.imread("./mask/" + name, cv2.IMREAD_UNCHANGED)
      segm = process_segment(segm)
      masked = orig.copy()
      for i in range(3):
          masked[:, :, i] = orig[:, :, i] * segm
      final_array = np.array([orig, masked]).reshape((orig.shape[0]*2, orig.shape[1], 3))
      print(final_array.shape)

      os.makedirs(name="./vis/", exist_ok=True)
      cv2.imwrite("./vis/" + name, final_array)


  for i in os.listdir("./png/"):
      visualize(i)
#+end_src

** Code to grow mask before doing object detection
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./segment.py
  def grow_mask(name):
      segm = cv2.imread("./mask/" + name, cv2.IMREAD_UNCHANGED)
      segm = process_segment(segm) * 255
      kernel1 = np.array(
          [
              [1, 1, 1, 1, 1],
              [1, 1, 1, 1, 1],
              [1, 1, 1, 1, 1],
              [1, 1, 1, 1, 1],
              [1, 1, 1, 1, 1],
          ]
      )
      segm = cv2.filter2D(src=segm, ddepth=-1, kernel=kernel1)
      cv2.imwrite("./mask/B_" + name, segm)


  for i in os.listdir("./png/"):
      grow_mask(i)
#+end_src

These produce almost perfect semantic segmentation masks, object detection at this stage is trivial.

* Track in video

** Convert videos to more standard format

*** Function to convert
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./convert.sh
  W(){
      ffmpeg -i "mov/${1}.MOV" "mp4/${1}.mp4" -c:v libx264
  }
#+end_src

*** Convert all videos
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./convert.sh
  W 'IMG_0629 weldnut'
  W 'IMG_0630-14 weldnut scan'
  W 'IMG_0631-14 weldnut scan'
  W 'IMG_0632-14 weldnut scan'
  W 'IMG_0633-12 weldnut scan'
#+end_src

** Extract first frame

*** Function to extract
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./convert.sh
  W(){
      mkdir -pv -- "mp4/${1}.dir"
      ffmpeg -i "mp4/${1}.mp4" -r 1 "mp4/${1}.dir/%d.png" 
  }
#+end_src

*** get from all videos
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./convert.sh
  W 'IMG_0629 weldnut'
  W 'IMG_0630-14 weldnut scan'
  W 'IMG_0631-14 weldnut scan'
  W 'IMG_0632-14 weldnut scan'
  W 'IMG_0633-12 weldnut scan'
#+end_src

* Code to extract bounding box
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./bbox.py
  from PIL import Image
  from ben2 import BEN_Base
  import cv2
  import numpy as np
  import os
  import torch


  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model = BEN_Base.from_pretrained("PramaLLC/BEN2")
  model.to(device).eval()


  def process_segment(img):
      return (img[:, :, 3] > 127).astype(np.uint8)


  def do_infer(image_PIL_input):
      foreground_PIL_output = model.inference(
          image_PIL_input,
          refine_foreground=False,
      )  # Refine foreground is an extract postprocessing step that increases inference time but can improve matting edges. The default value is False.
      return foreground_PIL_output


  def write_mask(path_file_image_input, path_file_mask_output):
      do_infer(image_PIL_input=Image.open(path_file_image_input)).save(
          path_file_mask_output
      )
      cv2.imwrite(
          path_file_mask_output,
          process_segment(img=cv2.imread(path_file_mask_output, cv2.IMREAD_UNCHANGED)),
      )


  def get_bbox(path_file_mask_input):
      mask_input = cv2.imread(path_file_mask_input, cv2.IMREAD_GRAYSCALE)
      rows = np.any(mask_input, axis=1)
      cols = np.any(mask_input, axis=0)
      r = np.where(rows)[0]
      c = np.where(cols)[0]

      if (r.flatten().shape[0] > 0) and (c.flatten().shape[0] > 0):
          rmin, rmax = r[[0, -1]]
          cmin, cmax = c[[0, -1]]
          return rmin.item(), cmin.item(), rmax.item(), cmax.item()
      else:
          return 0, 0, mask_input.shape[0] - 1, mask_input.shape[1] - 1


  def image_2_bbox(path_prefix_input):
      write_mask(
          path_file_image_input=path_prefix_input + ".png",
          path_file_mask_output=path_prefix_input + "_M.png",
      )
      y1, x1, y2, x2 = get_bbox(path_file_mask_input=path_prefix_input + "_M.png")
      h = y2 - y1
      w = x2 - x1
      with open(path_prefix_input + "_bbox.txt", "w", encoding="utf-8") as f:
          f.write(str(x1) + "," + str(y1) + "," + str(w) + "," + str(h))


  image_2_bbox(path_prefix_input="./mp4/IMG_0629 weldnut.dir/1")
  image_2_bbox(path_prefix_input="./mp4/IMG_0630-14 weldnut scan.dir/1")
  image_2_bbox(path_prefix_input="./mp4/IMG_0631-14 weldnut scan.dir/1")
  image_2_bbox(path_prefix_input="./mp4/IMG_0632-14 weldnut scan.dir/1")
  image_2_bbox(path_prefix_input="./mp4/IMG_0633-12 weldnut scan.dir/1")
#+end_src

* YOLO SAM for tracking nut in video

Yolo provides CLI tool which can be used to directly obtain object tracking masks on the video.
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./tmp.sh
  yolo predict model=sam2.1_t.pt source=1.mp4
#+end_src
Perhaps, we can get better results using [[https://github.com/yangchris11/samurai][samurai]] but I did not have the compute power or time to test this.

* COMMENT Samurai
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./track.sh
  . "${HOME}/BEN2/bin/activate"
  VIDEO_FILE="$(realpath "${1}")"
  TEXT_BBOX_FILE="$(realpath "${2}")"
  cd "${HOME}/GITHUB/yangchris11/samurai"
  python3 "${HOME}/GITHUB/yangchris11/samurai/scripts/demo_cpu.py" \
      --video_path "${VIDEO_FILE}" \
      --txt_path "${TEXT_BBOX_FILE}" \
  ;
#+end_src

./track.sh 'mp4/IMG_0629 weldnut.mp4' 'mp4/IMG_0629 weldnut.dir/1_bbox.txt'

* COMMENT SAM2 for segmentation and tracking

#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./sam2segmenttrack.py
  import torch

  import os

  PATH_DIR_HOME = os.environ.get("HOME", "/root")
  PATH_DIR_SAM = PATH_DIR_HOME + "/GITHUB/facebookresearch/sam2"
  PATH_FILE_MODEL_SAM_LARGE = PATH_DIR_SAM + "/checkpoints/sam2.1_hiera_large.pt"
  PATH_FILE_SAM_CONFIG = PATH_DIR_SAM + "/sam2/configs/sam2.1/sam2.1_hiera_l.yaml"

  import sys

  sys.path.append(PATH_DIR_SAM)

  from sam2.build_sam import build_sam2
  from sam2.sam2_image_predictor import SAM2ImagePredictor

  checkpoint = "./checkpoints/sam2.1_hiera_large.pt"
  model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
  predictor = SAM2ImagePredictor(
      build_sam2(model_cfg, PATH_FILE_MODEL_SAM_LARGE, device="cpu")
  )

  with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
      predictor.set_image(
          PATH_DIR_HOME + "/GITHUB/aravindhv10/weldnut/mp4/IMG_0629 weldnut.dir/1.png"
      )
      masks, _, _ = predictor.predict("nut")
#+end_src

* COMMENT WORK SPACE

** elisp
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  ;; (async-shell-command "./tmp.sh" "log" "err")
#+end_src
